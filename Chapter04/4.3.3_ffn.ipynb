{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 피드포워드 네트워크\n",
    "* 책의 코드가 동작하도록 원서의 내용에서 일부 코드를 추가했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서 형태: torch.Size([2, 10, 512])\n",
      "총 파라미터 수: 2,099,712\n",
      "출력 텐서 형태: torch.Size([2, 10, 512])\n",
      "입력과 출력 형태가 같은지: True\n",
      "\n",
      "네트워크 구조:\n",
      "FeedForward(\n",
      "  (l1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (l2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      "레이어별 가중치 형태:\n",
      "l1.weight: torch.Size([2048, 512])\n",
      "l1.bias: torch.Size([2048])\n",
      "l2.weight: torch.Size([512, 2048])\n",
      "l2.bias: torch.Size([512])\n",
      "\n",
      "학습 예시:\n",
      "Loss: 1.0602\n",
      "Gradient 계산 완료!\n",
      "l1.weight gradient norm: 0.1919\n",
      "l2.weight gradient norm: 0.4036\n",
      "입력 텐서 gradient norm: 0.0051\n",
      "\n",
      "옵티마이저 사용 예시:\n",
      "파라미터 업데이트 완료! 새로운 Loss: 1.0602\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer에서 사용되는 Position-wise Feed-Forward Network\n",
    "    \n",
    "    구조:\n",
    "    - Linear layer (input_dim → hidden_dim)\n",
    "    - SeLU activation function\n",
    "    - Linear layer (hidden_dim → input_dim)\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): 입력 차원 (보통 d_model과 같음)\n",
    "        hidden_dim (int): 은닉층 차원 (보통 input_dim의 4배)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        # 첫 번째 선형 변환: input_dim → hidden_dim\n",
    "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # 두 번째 선형 변환: hidden_dim → input_dim (원래 차원으로 복원)\n",
    "        self.l2 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        # ReLU 활성화 함수 (일반적으로 Transformer에서 많이 사용)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 또는 GELU를 사용하려면 아래 주석을 해제하세요 (최신 Transformer에서 선호)\n",
    "        # self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 과정\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): 입력 텐서 (batch_size, seq_len, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: 출력 텐서 (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        # 첫 번째 선형 변환 후 ReLU 활성화\n",
    "        x = self.relu(self.l1(x))\n",
    "        \n",
    "        # GELU를 사용하려면 위 줄을 아래로 교체하세요:\n",
    "        # x = F.gelu(self.l1(x))\n",
    "        \n",
    "        # 두 번째 선형 변환 (활성화 함수 없음)\n",
    "        x = self.l2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_dim = 512    # 입력/출력 차원 (d_model)\n",
    "hidden_dim = 2048  # 은닉층 차원 (보통 d_model * 4)\n",
    "batch_size = 2     # 배치 크기\n",
    "seq_len = 10       # 시퀀스 길이\n",
    "\n",
    "# 더미 입력 데이터 생성 (예: Transformer의 attention layer 출력)\n",
    "inputs = torch.randn(batch_size, seq_len, input_dim)\n",
    "print(f\"입력 텐서 형태: {inputs.shape}\")\n",
    "\n",
    "# FeedForward 네트워크 인스턴스 생성\n",
    "feed_forward = FeedForward(input_dim, hidden_dim)\n",
    "\n",
    "# 모델 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in feed_forward.parameters())\n",
    "print(f\"총 파라미터 수: {total_params:,}\")\n",
    "\n",
    "# 추론 모드에서 순전파 실행\n",
    "feed_forward.eval()  # 평가 모드로 설정\n",
    "with torch.no_grad():  # 추론 모드 (gradient 계산 안함)\n",
    "    outputs_inference = feed_forward(inputs)\n",
    "\n",
    "print(f\"출력 텐서 형태: {outputs_inference.shape}\")\n",
    "print(f\"입력과 출력 형태가 같은지: {inputs.shape == outputs_inference.shape}\")\n",
    "\n",
    "# 네트워크 구조 출력\n",
    "print(f\"\\n네트워크 구조:\")\n",
    "print(feed_forward)\n",
    "\n",
    "# 각 레이어별 가중치 형태 확인\n",
    "print(f\"\\n레이어별 가중치 형태:\")\n",
    "print(f\"l1.weight: {feed_forward.l1.weight.shape}\")\n",
    "print(f\"l1.bias: {feed_forward.l1.bias.shape}\")\n",
    "print(f\"l2.weight: {feed_forward.l2.weight.shape}\")\n",
    "print(f\"l2.bias: {feed_forward.l2.bias.shape}\")\n",
    "\n",
    "# 간단한 예시: 학습 모드에서의 gradient 계산\n",
    "print(f\"\\n학습 예시:\")\n",
    "feed_forward.train()  # 학습 모드로 설정\n",
    "\n",
    "# gradient 계산을 위해 입력 텐서에 requires_grad=True 설정\n",
    "inputs_train = torch.randn(batch_size, seq_len, input_dim, requires_grad=True)\n",
    "\n",
    "# 순전파 (gradient 계산 활성화)\n",
    "outputs = feed_forward(inputs_train)\n",
    "\n",
    "# 가상의 target과 loss 계산\n",
    "target = torch.randn_like(outputs)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(outputs, target)\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "print(\"Gradient 계산 완료!\")\n",
    "\n",
    "# 네트워크 파라미터의 gradient 확인\n",
    "if feed_forward.l1.weight.grad is not None:\n",
    "    print(f\"l1.weight gradient norm: {feed_forward.l1.weight.grad.norm().item():.4f}\")\n",
    "    print(f\"l2.weight gradient norm: {feed_forward.l2.weight.grad.norm().item():.4f}\")\n",
    "\n",
    "# 입력 텐서의 gradient도 확인 (requires_grad=True로 설정했기 때문)\n",
    "if inputs_train.grad is not None:\n",
    "    print(f\"입력 텐서 gradient norm: {inputs_train.grad.norm().item():.4f}\")\n",
    "\n",
    "# Optimizer 사용 예시\n",
    "print(f\"\\n옵티마이저 사용 예시:\")\n",
    "optimizer = torch.optim.Adam(feed_forward.parameters(), lr=0.001)\n",
    "\n",
    "# Gradient 초기화\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 새로운 forward pass\n",
    "outputs = feed_forward(inputs_train)\n",
    "loss = criterion(outputs, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# 파라미터 업데이트\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"파라미터 업데이트 완료! 새로운 Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
