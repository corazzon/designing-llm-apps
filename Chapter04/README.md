# Chapter 4

## Exercises

* 4.2_nltk.ipynb [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/corazzon/designing-llm-apps/blob/main/Chapter04/4.2_nltk.ipynb)
* 4.3_transformers.ipynb [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/corazzon/designing-llm-apps/blob/main/Chapter04/4.3_transformers.ipynb)
* 4.3.2_self_attention.ipynb [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/corazzon/designing-llm-apps/blob/main/Chapter04/4.3.2_self_attention.ipynb)
* 4.3.3_ffn.ipynb [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/corazzon/designing-llm-apps/blob/main/Chapter04/4.3.3_ffn.ipynb)
* 4.3.4_layer_norm.ipynb [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/corazzon/designing-llm-apps/blob/main/Chapter04/4.3.4_layer_norm.ipynb)
* 4.7.1_lm_next_token.ipynb [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/corazzon/designing-llm-apps/blob/main/Chapter04/4.7.1_lm_next_token.ipynb)
* masking_strategies.ipynb [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/corazzon/designing-llm-apps/blob/main/Chapter04/masking_strategies.ipynb)

## References

* "Neural Network Methods for Natural Language Processing", https://oreil.ly/oDc6x
* Ashish Vaswani et al., "Attention Is All You Need", 12 Jun 2017, https://arxiv.org/abs/1706.03762
* "Softmax Activation Function with Python", https://oreil.ly/b6gHV
* Ashish Vaswani et al., "Attention Is All You Need", 12 Jun 2017, https://arxiv.org/abs/1706.03762
* Press et al., "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation", 27 Aug 2021, https://arxiv.org/abs/2108.12409
* Su et al., "Enhanced Transformer with Rotary Position Embedding", 20 Apr 2021, https://arxiv.org/abs/2104.09864
* Kazemnejad et al., "The Impact of Positional Encoding on Length Generalization in Transformers", 31 May 2023, https://arxiv.org/abs/2305.19466
* "Feedforward neural network", https://oreil.ly/Bdphg
* "ReLU — PyTorch documentation", https://oreil.ly/KUqtP
* "GeLU — PyTorch documentation", https://oreil.ly/MSDKE
* "Activation Functions in Neural Networks [12 Types & Use Cases]", https://oreil.ly/NfOb0
* Komatsuzaki et al., "SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS", 17 Feb 2023, https://arxiv.org/pdf/2212.05055
* Yi Tay et al, "UL2: Unifying Language Learning Paradigms", 28 Feb 2023, https://arxiv.org/pdf/2205.05131
* https://database.lichess.org/
* "PGN Viewer: Nodirbek Abdusattorov", https://oreil.ly/H3yOs
* https://www.chesshouse.com/pages/chess-rules
