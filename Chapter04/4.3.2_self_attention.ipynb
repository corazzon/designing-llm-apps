{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 셀프어텐션\n",
    "* 책의 코드가 동작하도록 원서의 내용에서 일부 코드를 추가했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([2, 10, 32])\n",
      "Key shape: torch.Size([2, 10, 32])\n",
      "Value shape: torch.Size([2, 10, 32])\n",
      "\n",
      "Attention scores shape: torch.Size([2, 10, 10])\n",
      "Normalized attention scores shape: torch.Size([2, 10, 10])\n",
      "Output shape: torch.Size([2, 10, 32])\n",
      "\n",
      "First batch attention weights (first 5x5):\n",
      "tensor([[0.1106, 0.1001, 0.0810, 0.1316, 0.0859],\n",
      "        [0.0841, 0.0662, 0.1048, 0.0934, 0.2182],\n",
      "        [0.0976, 0.0885, 0.1467, 0.1015, 0.0777],\n",
      "        [0.0847, 0.1341, 0.1221, 0.1194, 0.1067],\n",
      "        [0.0815, 0.0739, 0.1473, 0.0705, 0.0844]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_model = 64  # embedding dimension\n",
    "d_k = 32      # key/query dimension\n",
    "d_v = 32      # value dimension\n",
    "\n",
    "# 입력 임베딩 생성 (예시)\n",
    "input_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Weight 행렬들 정의\n",
    "wQ = nn.Linear(d_model, d_k, bias=False)\n",
    "wK = nn.Linear(d_model, d_k, bias=False)\n",
    "wV = nn.Linear(d_model, d_v, bias=False)\n",
    "\n",
    "# Self-Attention 계산\n",
    "q = wQ(input_embeddings)  # (batch_size, seq_len, d_k)\n",
    "k = wK(input_embeddings)  # (batch_size, seq_len, d_k)\n",
    "v = wV(input_embeddings)  # (batch_size, seq_len, d_v)\n",
    "\n",
    "print(f\"Query shape: {q.shape}\")\n",
    "print(f\"Key shape: {k.shape}\")\n",
    "print(f\"Value shape: {v.shape}\")\n",
    "\n",
    "# Attention 점수 계산\n",
    "dim_k = k.size(-1)\n",
    "attn_scores = torch.matmul(q, k.transpose(-2, -1))  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "# 스케일링\n",
    "scaled_attn_scores = attn_scores / torch.sqrt(torch.tensor(dim_k, dtype=torch.float32))\n",
    "\n",
    "# Softmax 적용\n",
    "normalized_attn_scores = F.softmax(scaled_attn_scores, dim=-1)\n",
    "\n",
    "# 최종 출력 계산\n",
    "output = torch.matmul(normalized_attn_scores, v)  # (batch_size, seq_len, d_v)\n",
    "\n",
    "print(f\"\\nAttention scores shape: {attn_scores.shape}\")\n",
    "print(f\"Normalized attention scores shape: {normalized_attn_scores.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# 첫 번째 배치의 attention 가중치 시각화\n",
    "print(f\"\\nFirst batch attention weights (first 5x5):\")\n",
    "print(normalized_attn_scores[0, :5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
