# Chapter 5

* 5장 Using LIT with Gemma tutorial : https://oreil.ly/zcsLu

## References 
* https://oreil.ly/llm-playbooks
* https://openai.com/api/
* https://ai.google.dev/
* https://www.anthropic.com/claude
* https://cohere.com/command
* https://www.ai21.com/
* https://aws.amazon.com/ko/bedrock/
* "SageMaker JumpStart pretrained models - Amazon SageMaker AI", https://oreil.ly/e0a59
* "Model Garden on Vertex AI | Google Cloud", https://cloud.google.com/model-garden
* "Azure OpenAI in Foundry Models | Microsoft Azure", https://oreil.ly/Ag1r5
* "Together AI – The AI Acceleration Cloud - Fast Inference, Fine-Tuning & Training", https://www.together.ai/
* https://www.cerebras.net/
* https://www.eleuther.ai/
* "BigScience Research Workshop", https://bigscience.huggingface.co/
* "Technology Innovation Institute", https://www.tii.ae/
* "Introducing the Technology Innovation Institute’s Falcon 3 Making Advanced AI accessible and Available to Everyone, Everywhere", https://falconllm.tii.ae/falcon-models.html
* "GLM-130B: An Open Bilingual Pre-Trained Model | GLM-130B", https://oreil.ly/K0_zX
* "FLAN/flan/v2 at main · google-research/FLAN · GitHub", https://oreil.ly/YJ_Xr
* Bai et al., "Constitutional AI: Harmlessness from AI Feedback", 15 Dec 2022, https://arxiv.org/abs/2212.08073
* "GitHub - EleutherAI/lm-evaluation-harness: A framework for few-shot evaluation of language models.", https://oreil.ly/SiOXq
* "Hugging Face Hub documentation", https://oreil.ly/IHd22
* Chung et al., "Scaling Instruction-Finetuned Language Models", 20 Oct 2022, https://arxiv.org/abs/2210.11416
* "Needle In A Haystack Experimental Evaluation", https://oreil.ly/aop_Q
* "ProsusAI/finbert · Hugging Face", https://oreil.ly/uKUAp
* Zhou et al., "UniversalNER A case study on targeted distillation from LLMs", https://universal-ner.github.io/
* OSI et al., "The Open Source Definition – Open Source Initiative", https://opensource.org/osd
* "BigScience OpenRAIL-M", https://oreil.ly/2UVMe
* "About CC Licenses - Creative Commons", https://oreil.ly/PQy6D
* "Kannada LLAMA | Tensoic", https://www.tensoic.com/blog/kannada-llama/
* "GitHub - EleutherAI/lm-evaluation-harness: A framework for few-shot evaluation of language models.", https://oreil.ly/SiOXq
* "Hugging Face Hub documentation", https://oreil.ly/IHd22
* https://oreil.ly/fwVEC
* https://oreil.ly/dI87I
* "What's going on with the Open LLM Leaderboard?", https://oreil.ly/QrBX4
* https://lmsys.org/
* https://lmarena.ai/?arena
* "Chatbot Arena Leaderboard - a Hugging Face Space by lmarena-ai", https://oreil.ly/Y6zmN
* Wu et al., "Style Over Substance: Evaluation Biases for Large Language Models", 6 Jul 2023, https://arxiv.org/abs/2307.03025
* Touvron et al., "Llama 2: Open Foundation and Fine-Tuned Chat Models", 18 Jul 2023, https://arxiv.org/abs/2307.09288
* https://oreil.ly/t6iPQ
* "Accelerate Big Model Inference: How Does it Work? - YouTube", https://oreil.ly/J8duc
* "ollama/docs/modelfile.md", https://oreil.ly/ba-1u
* Puigcerver et al., "From Sparse to Soft Mixtures of Experts", 2 Aug 2023, https://arxiv.org/abs/2308.00951
* Wang et al., "Self-Consistency Improves Chain of Thought Reasoning in Language Models", 21 Mar 2022, https://arxiv.org/abs/2203.11171
* https://github.com/1rgs/jsonformer
* https://github.com/guidance-ai/guidance
* https://oreil.ly/tUrIt
* https://pair-code.github.io/lit/
* https://oreil.ly/llm-playbooks
* "lit_gemma", https://oreil.ly/zcsLu
* Bricken et al., "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning", 4 Oct 2023, https://oreil.ly/hLdVN 
* "Dictionary Learning Features", https://oreil.ly/2YvE6
