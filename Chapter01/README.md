# Chapter 1

## Notebooks

* [1.6_openai_api_dog_and_elephant.ipynb](https://colab.research.google.com/github/your-username/designing-llm-apps/blob/main/Chapter01/1.6_openai_api_dog_and_elephant.ipynb)
* [ChatwithyourPDF.ipynb](https://colab.research.google.com/github/your-username/designing-llm-apps/blob/main/Chapter01/ChatwithyourPDF.ipynb)
* [Loading_LLMs_Basic_llama2.ipynb](https://colab.research.google.com/github/your-username/designing-llm-apps/blob/main/Chapter01/getting_started/colab/Loading_LLMs_Basic_llama2.ipynb)
* [Loading_Models_with_BnB_in_Colab_Free_Tier.ipynb](https://colab.research.google.com/github/your-username/designing-llm-apps/blob/main/Chapter01/getting_started/colab/Loading_Models_with_BnB_in_Colab_Free_Tier.ipynb)
* [Poetry_Installation_On_Colab.ipynb](https://colab.research.google.com/github/your-username/designing-llm-apps/blob/main/Chapter01/getting_started/colab/Poetry_Installation_On_Colab.ipynb)

## Chapter 1 Exercises

### Chat with your PDF: DocuBuddy 
- Jupyter Notebook Code
- Streamlit deployment: https://docubuddy.streamlit.app/
- Gradio deployment: [at2507/DocuBuddy](https://huggingface.co/spaces/at2507/DocuBuddy)


## References

* Dwarkesh Patel, "Why next-token prediction is enough for AGI - Ilya Sutskever (OpenAI Chief Scientist)", https://www.youtube.com/watch?v=YEUclZdj_Sc 
* Yann Lecun, Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416, https://www.youtube.com/watch?v=5t1vTLU7s40&t=145s 
* Adam Karvonen, "Chess-GPT's Internal World Model", 3 Jan 2024, https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html 
* https://huggingface.co/ctheodoris/Geneformer
* https://x.com/WilliamWangNLP/status/1835040381668675747
* Kaplan et al "Scaling Laws for Neural Language Models", 23 Jan 2020, https://arxiv.org/pdf/2001.08361  
* Hoffmann et al., "Training Compute-Optimal Large Language Models", 29 Mar 2022, https://arxiv.org/pdf/2203.15556 
* Wei et al., "Emergent Abilities of Large Language Models", 26 Oct 2022, https://arxiv.org/pdf/2206.07682
* Schaeffer et al., "Are Emergent Abilities of Large Language Models a
Mirage?", 22 May 2023, https://arxiv.org/pdf/2304.15004
* Kaplan et al "Scaling Laws for Neural Language Models", 23 Jan 2020, https://arxiv.org/pdf/2001.08361 
* https://www.goodreads.com/book/show/6474461
* https://tallinzen.net/media/readings/chomsky_syntactic_structures.pdf
* https://www.regular-expressions.info/
* https://web.njit.edu/~ronkowit/eliza.html
* "Computational Linguistics", Jan 1966, https://hackaday.com/wp-content/uploads/2024/02/WEIZENBAUM-1966-ELIZA-A-Computer-Program-For-the-Study-of-Natural-Language-Communication-Between-Man-And-Machine.pdf 
* https://github.com/wadetb/eliza
* https://en.wikiquote.org/wiki/Fred_Jelinek
* https://nlp.stanford.edu/fsnlp/
* Linguistic Fundamentals for Natural Language Processing, https://oreil.ly/hWR8S
* Linguistic Fundamentals for Natural Language Processing II, https://oreil.ly/7liiS
* 심층 학습 , 제이펍, 류광 옮김 https://www.deeplearningbook.org/ 
* https://d2l.ai/
* https://oreil.ly/MCOp4
* Vaswani et al., "Attention Is All You Need, 12 Jun 2017, https://arxiv.org/abs/1706.03762 
* Radford et al., "Improving Language Understanding by Generative Pre-Training", 2018,  https://oreil.ly/dFPSE 
* Radford et al., "Language Models are Unsupervised Multitask Learners", 2019, https://oreil.ly/JL-VO  
* Brown et al., "Language Models are Few-Shot Learners", 2020, https://arxiv.org/pdf/2005.14165 
* OpenAI, "GPT-4 Technical Report", 2023, https://arxiv.org/pdf/2303.08774
* "Learning to reason with LLMs", https://openai.com/index/learning-to-reason-with-llms/ 
* Suhas Pai,"Twice as Many Companies Get AI Questions From Analysts", 26 Mar 2024, https://oreil.ly/_mTAs 
* https://www.jasper.ai/
* https://www.copy.ai/
* Fabio Chiusano, "Two minutes NLP — 33 important NLP tasks explained", 7 Dec 2021, https://oreil.ly/_11rN 
* https://erudite.cc/
* Pryzant et al., "Automatic Prompt Optimization with "Gradient Descent" and Beam Search", 4 May 2023, https://arxiv.org/abs/2305.03495
* Shin et al., "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", 29 Oct 2020, https://arxiv.org/abs/2010.15980
* https://learnprompting.org/docs/introduction
* Sprague et al., "TO COT OR NOT TO COT? CHAIN-OF-THOUGHT HELPS
MAINLY ON MATH AND SYMBOLIC REASONING", 7 May 2025, https://arxiv.org/pdf/2409.12183
* https://gandalf.lakera.ai/
* https://www.lakera.ai/
* https://arxiv.org/pdf/2307.11760
* https://oreil.ly/DAa66
* https://oreil.ly/NRwIb  
* https://github.com/features/copilot 
* https://oreil.ly/ivvv_ 
* Thrush et al., "I am a Strange Dataset: Metalinguistic Tests for Language Models", 6 Aug 2024, https://arxiv.org/pdf/2401.05300
* https://github.com/TristanThrush/i-am-a-strange-dataset
* https://arxiv.org/pdf/2403.05750v1
* https://oreil.ly/hetca
* https://oreil.ly/5xECI
* Shaib et al., "Detection and Measurement of Syntactic Templates in Generated Text", 6 Oct 2024, https://arxiv.org/pdf/2407.00211
* Shaib et al., "Standardizing the Measurement of Text Diversity: A Tool and Comparative Analysis", 21 Mar 2025, https://arxiv.org/html/2403.00553v2
* https://oreil.ly/llm-playbooks
