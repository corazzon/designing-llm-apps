{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지정된 버전의 라이브러리들이 설치되어 있는지 확인\n",
    "# !pip install transformers==4.35.0 accelerate==0.24.0 peft==0.6.0 \\\n",
    "# bitsandbytes==0.41.0 trl==0.7.4 datasets torch\n",
    "# !uv pip install transformers==4.35.0 accelerate==0.24.0 peft==0.6.0 \\\n",
    "# bitsandbytes==0.41.0 trl==0.7.4 datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809132d1289b4084a4e9994fee96a52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ae6e33985f440db41918cac8554b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08eaffd0c76d4a38b8b6c84fede224eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파인튜닝 데이터셋 로드 완료: 5 샘플\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/corazzon/codes/.venv/lib/python3.13/site-packages/datasets/download/streaming_download_manager.py:765: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     38\u001b[39m lora_params = LoraConfig(\n\u001b[32m     39\u001b[39m     r=\u001b[32m64\u001b[39m,                                    \u001b[38;5;66;03m# LoRA 랭크 차원\u001b[39;00m\n\u001b[32m     40\u001b[39m     lora_alpha=\u001b[32m16\u001b[39m,                           \u001b[38;5;66;03m# LoRA 스케일링 계수\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     task_type=\u001b[33m\"\u001b[39m\u001b[33mCAUSAL_LM\u001b[39m\u001b[33m\"\u001b[39m                    \u001b[38;5;66;03m# 작업 유형\u001b[39;00m\n\u001b[32m     45\u001b[39m )\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# 훈련 설정 구성 (TrainingArguments)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m train_params = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./model_outputs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# 출력 디렉터리\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# 훈련 에포크 수\u001b[39;49;00m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# 장치당 훈련 배치 크기 (메모리 절약)\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# 장치당 평가 배치 크기\u001b[39;49;00m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# 그레이디언트 누적 스텝 수\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpaged_adamw_32bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# 32비트 페이징 AdamW 옵티마이저\u001b[39;49;00m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# 학습률 (LoRA에 적합하게 조정)\u001b[39;49;00m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# 가중치 감쇠\u001b[39;49;00m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# 워밍업 비율\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# 그레이디언트 클리핑\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_by_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# 길이별 그룹화\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# 코사인 스케줄러\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                               \u001b[49m\u001b[38;5;66;43;03m# FP16 사용 (호환성)\u001b[39;49;00m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# 그레이디언트 체크포인팅\u001b[39;49;00m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\u001b[38;5;66;43;03m# 저장 간격\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# 로깅 간격\u001b[39;49;00m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# 평가 전략\u001b[39;49;00m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# 저장할 체크포인트 수\u001b[39;49;00m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# 리포트 비활성화\u001b[39;49;00m\n\u001b[32m     68\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# 모델과 토크나이저 로드를 위한 대안 설정 (Llama-2가 접근 제한이 있는 경우)\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# LLaMA-2 모델 시도\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:117\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, neftune_noise_alpha)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codes/.venv/lib/python3.13/site-packages/transformers/training_args.py:1448\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[32m   (...)\u001b[39m\u001b[32m   1446\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.fp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp16_full_eval)\n\u001b[32m   1447\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1449\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1451\u001b[39m     )\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1454\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[32m   (...)\u001b[39m\u001b[32m   1462\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.bf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16_full_eval)\n\u001b[32m   1463\u001b[39m ):\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1465\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1466\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1467\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX)."
     ]
    }
   ],
   "source": [
    "# 데이터셋과 훈련 관련 구성 요소 불러오기\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, BitsAndBytesConfig\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# 샘플 파인튜닝 데이터 생성\n",
    "sample_data = {\n",
    "    'text': [\n",
    "        \"질문: 파이썬에서 리스트란 무엇인가요?\\n답변: 리스트는 순서가 있는 데이터 모음으로, 대괄호로 표현합니다.\",\n",
    "        \"질문: 머신러닝이란 무엇인가요?\\n답변: 머신러닝은 데이터로부터 패턴을 학습하여 예측하는 AI 기술입니다.\",\n",
    "        \"질문: 파이썬에서 함수를 정의하는 방법은?\\n답변: def 키워드를 사용하여 함수를 정의할 수 있습니다.\",\n",
    "        \"질문: 딥러닝과 머신러닝의 차이점은?\\n답변: 딥러닝은 인공신경망을 사용하는 머신러닝의 한 분야입니다.\",\n",
    "        \"질문: 자연어 처리란 무엇인가요?\\n답변: 자연어 처리는 컴퓨터가 인간의 언어를 이해하고 처리하는 기술입니다.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# CSV 파일 생성 및 데이터셋 로드\n",
    "df = pd.DataFrame(sample_data)\n",
    "csv_path = './finetune_data.csv'\n",
    "df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "tune_data = load_dataset(\"csv\", data_files=csv_path)\n",
    "print(f\"파인튜닝 데이터셋 로드 완료: {len(tune_data['train'])} 샘플\")\n",
    "\n",
    "# 4비트 양자화 설정 (BitsAndBytesConfig)\n",
    "# quantize_params = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,                       # 4비트 로딩 활성화\n",
    "#     bnb_4bit_compute_dtype=torch.float16,    # 연산용 dtype은 float16\n",
    "#     bnb_4bit_quant_type='nf4',               # NF4 양자화 형식 사용\n",
    "#     bnb_4bit_use_double_quant=False          # 이중 양자화 비활성화\n",
    "# )\n",
    "# 4비트 양자화 설정 (BitsAndBytesConfig) - CPU 환경에서는 비활성화\n",
    "quantize_params = None  # CPU 환경에서는 양자화 비활성화\n",
    "# LoRA 설정 (LoraConfig)\n",
    "lora_params = LoraConfig(\n",
    "    r=64,                                    # LoRA 랭크 차원\n",
    "    lora_alpha=16,                           # LoRA 스케일링 계수\n",
    "    lora_dropout=0.1,                        # LoRA 드롭아웃 확률\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],     # 적용할 모듈 지정\n",
    "    bias=\"none\",                             # 바이어스 설정\n",
    "    task_type=\"CAUSAL_LM\"                    # 작업 유형\n",
    ")\n",
    "\n",
    "# 훈련 설정 구성 (TrainingArguments)\n",
    "train_params = TrainingArguments(\n",
    "    output_dir='./model_outputs',            # 출력 디렉터리\n",
    "    num_train_epochs=3,                      # 훈련 에포크 수\n",
    "    per_device_train_batch_size=2,           # 장치당 훈련 배치 크기 (메모리 절약)\n",
    "    per_device_eval_batch_size=2,            # 장치당 평가 배치 크기\n",
    "    gradient_accumulation_steps=4,           # 그레이디언트 누적 스텝 수\n",
    "    optim=\"paged_adamw_32bit\",               # 32비트 페이징 AdamW 옵티마이저\n",
    "    learning_rate=2e-4,                      # 학습률 (LoRA에 적합하게 조정)\n",
    "    weight_decay=0.01,                       # 가중치 감쇠\n",
    "    warmup_ratio=0.03,                       # 워밍업 비율\n",
    "    max_grad_norm=1.0,                       # 그레이디언트 클리핑\n",
    "    group_by_length=True,                    # 길이별 그룹화\n",
    "    lr_scheduler_type=\"cosine\",              # 코사인 스케줄러\n",
    "    fp16=True,                               # FP16 사용 (호환성)\n",
    "    gradient_checkpointing=True,             # 그레이디언트 체크포인팅\n",
    "    save_steps=50,                           # 저장 간격\n",
    "    logging_steps=10,                        # 로깅 간격\n",
    "    evaluation_strategy=\"no\",                # 평가 전략\n",
    "    save_total_limit=2,                      # 저장할 체크포인트 수\n",
    "    report_to=\"none\"                         # 리포트 비활성화\n",
    ")\n",
    "\n",
    "# 모델과 토크나이저 로드를 위한 대안 설정 (Llama-2가 접근 제한이 있는 경우)\n",
    "try:\n",
    "    # LLaMA-2 모델 시도\n",
    "    model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantize_params,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    print(\"LLaMA-2 모델 로드 성공!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LLaMA-2 로드 실패: {e}\")\n",
    "    print(\"대안 모델을 사용합니다...\")\n",
    "    \n",
    "    # 대안: 공개 모델 사용\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    model_name = 'microsoft/DialoGPT-medium'\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantize_params,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"대안 모델 로드 성공!\")\n",
    "\n",
    "# SFTTrainer를 이용한 지도 학습 기반 파인 튜닝 설정\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,                             # 모델\n",
    "    args=train_params,                       # 훈련 파라미터\n",
    "    train_dataset=tune_data['train'],        # 훈련 데이터셋\n",
    "    tokenizer=tokenizer,                     # 토크나이저\n",
    "    peft_config=lora_params,                 # PEFT 설정\n",
    "    max_seq_length=512,                      # 시퀀스 최대 길이\n",
    "    dataset_text_field='text',               # 텍스트 필드 지정\n",
    "    packing=False                            # 패킹 비활성화 (안정성)\n",
    ")\n",
    "\n",
    "print(\"SFTTrainer 설정 완료!\")\n",
    "print(f\"훈련 데이터 크기: {len(tune_data['train'])}\")\n",
    "print(f\"배치 크기: {train_params.per_device_train_batch_size}\")\n",
    "print(f\"누적 스텝: {train_params.gradient_accumulation_steps}\")\n",
    "print(f\"유효 배치 크기: {train_params.per_device_train_batch_size * train_params.gradient_accumulation_steps}\")\n",
    "\n",
    "# 훈련 실행 준비\n",
    "print(\"\\n훈련을 시작하려면 다음 코드를 실행하세요:\")\n",
    "print(\"sft_trainer.train()\")\n",
    "print(\"\\n훈련 완료 후 모델 저장:\")\n",
    "print(\"sft_trainer.model.save_pretrained('./fine_tuned_model')\")\n",
    "print(\"tokenizer.save_pretrained('./fine_tuned_model')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
