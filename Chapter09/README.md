# Chapter 9

## Notebooks

* [llama_quantization.ipynb](https://colab.research.google.com/github/your-username/designing-llm-apps/blob/main/Chapter09/llama_quantization.ipynb)
* [quantization_analysis.ipynb](https://colab.research.google.com/github/your-username/designing-llm-apps/blob/main/Chapter09/quantization_analysis.ipynb)

## References 

* Zhou et al., "A Survey on Efficient Inference for Large Language Models", 19 Jul 2024, https://arxiv.org/pdf/2404.14294
* Liu et al., "FastBERT: a Self-distilling BERT with Adaptive Inference Time", 5 Apr 2020, https://arxiv.org/abs/2004.02178
* Sun et al., "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation", 3 Mar 2022, https://arxiv.org/abs/2203.01670
* https://github.com/piesauce/llm-playbooks
* Schuster et al., "Confident Adaptive Language Modeling", 25 Oct 2022, https://arxiv.org/pdf/2207.07061
* https://oreil.ly/rgiHZ
* Xu et al., "A Survey on Knowledge Distillation of Large Language Models", 21 Oct 2024, https://arxiv.org/pdf/2402.13116
* Zhou et al., "LIMA: Less Is More for Alignment", 18 May 2023, https://arxiv.org/abs/2305.11206
* Xu et al., "A Survey on Knowledge Distillation of Large Language Models", 21 Oct 2024, https://arxiv.org/pdf/2402.13116
* Liang et al., "Less is More: Task-aware Layer-wise Distillation for Language Model Compression", 2023, https://oreil.ly/g-C4L
* https://arxiv.org/pdf/2312.09390
* Zhou et al., "DistillSpec: Improving Speculative Decoding via Knowledge Distillation", 12 Oct 2023, https://arxiv.org/abs/2310.08461
* He et al., "REST: Retrieval-Based Speculative Decoding", 14 Nov 2024, https://arxiv.org/abs/2311.08252
* Cai et al., "MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads", 14 Jun 2024, https://arxiv.org/pdf/2401.10774
* "Demystifying Floating Point Precision", https://oreil.ly/uCYYl
* Maarten Grootendors, "A Visual Guide to Quantization", https://oreil.ly/bpi3b
* https://oreil.ly/llm-playbooks
