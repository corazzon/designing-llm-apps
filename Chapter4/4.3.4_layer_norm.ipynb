{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.4 층 정규화\n",
    "* 책의 코드가 동작하도록 원서의 내용에서 일부 코드를 추가했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서 형태: torch.Size([2, 10, 512])\n",
      "입력 데이터 통계:\n",
      "  - 전체 평균: 1.9785\n",
      "  - 전체 표준편차: 3.0473\n",
      "  - 마지막 차원 평균 (첫 번째 샘플): 2.1421\n",
      "  - 마지막 차원 표준편차 (첫 번째 샘플): 2.9843\n",
      "\n",
      "LayerNorm 파라미터:\n",
      "  - gamma 형태: torch.Size([512])\n",
      "  - beta 형태: torch.Size([512])\n",
      "  - epsilon: 1e-05\n",
      "\n",
      "출력 텐서 형태: torch.Size([2, 10, 512])\n",
      "출력 데이터 통계:\n",
      "  - 전체 평균: 0.0000\n",
      "  - 전체 표준편차: 1.0000\n",
      "  - 마지막 차원 평균 (첫 번째 샘플): -0.000000\n",
      "  - 마지막 차원 표준편차 (첫 번째 샘플): 1.000978\n",
      "\n",
      "PyTorch 내장 LayerNorm 비교:\n",
      "  - 내장 LayerNorm 평균: 0.000000\n",
      "  - 내장 LayerNorm 표준편차: 1.000978\n",
      "\n",
      "정규화 검증 (처음 3개 샘플):\n",
      "  샘플[0][0] - 평균: -0.000000, 표준편차: 1.000978\n",
      "  샘플[0][1] - 평균: 0.000000, 표준편차: 1.000977\n",
      "  샘플[0][2] - 평균: 0.000000, 표준편차: 1.000977\n",
      "  샘플[1][0] - 평균: 0.000000, 표준편차: 1.000978\n",
      "  샘플[1][1] - 평균: -0.000000, 표준편차: 1.000978\n",
      "  샘플[1][2] - 평균: 0.000000, 표준편차: 1.000978\n",
      "\n",
      "학습 가능한 파라미터:\n",
      "  gamma: torch.Size([512]), requires_grad=True\n",
      "  beta: torch.Size([512]), requires_grad=True\n",
      "\n",
      "학습 예시:\n",
      "초기 Loss: 2.0129\n",
      "한 스텝 후 Loss: 2.0129\n",
      "Gamma 변화: tensor([0.9900, 0.9900, 0.9900, 0.9900, 0.9900], grad_fn=<SliceBackward0>)...\n",
      "Beta 변화: tensor([ 0.0100, -0.0100, -0.0100,  0.0100,  0.0100], grad_fn=<SliceBackward0>)...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization 구현\n",
    "    \n",
    "    각 입력 샘플에 대해 마지막 차원을 기준으로 정규화를 수행합니다.\n",
    "    Transformer 아키텍처에서 핵심적으로 사용되는 정규화 기법입니다.\n",
    "    \n",
    "    Args:\n",
    "        dimension (int): 정규화할 차원의 크기\n",
    "        gamma (torch.Tensor, optional): 스케일 파라미터 (기본값: 1로 초기화)\n",
    "        beta (torch.Tensor, optional): 시프트 파라미터 (기본값: 0으로 초기화)\n",
    "        epsilon (float): 수치적 안정성을 위한 작은 값 (기본값: 1e-5)\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension, gamma=None, beta=None, epsilon=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # gamma (scale parameter): 학습 가능한 파라미터로 1로 초기화\n",
    "        self.gamma = gamma if gamma is not None else nn.Parameter(torch.ones(dimension))\n",
    "        \n",
    "        # beta (shift parameter): 학습 가능한 파라미터로 0으로 초기화\n",
    "        self.beta = beta if beta is not None else nn.Parameter(torch.zeros(dimension))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        LayerNorm 순전파\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): 입력 텐서 (batch_size, seq_len, dimension)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: 정규화된 출력 텐서 (입력과 같은 형태)\n",
    "        \"\"\"\n",
    "        # 마지막 차원에 대해 평균 계산 (keepdim=True로 차원 유지)\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        \n",
    "        # 마지막 차원에 대해 분산 계산 (unbiased=False: 모집단 분산)\n",
    "        variance = x.var(-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # 정규화: (x - mean) / sqrt(variance + epsilon)\n",
    "        x_normalized = (x - mean) / torch.sqrt(variance + self.epsilon)\n",
    "        \n",
    "        # 스케일링과 시프트: gamma * x_normalized + beta\n",
    "        return self.gamma * x_normalized + self.beta\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "embedding_dim = 512  # 임베딩 차원\n",
    "batch_size = 2       # 배치 크기\n",
    "seq_len = 10         # 시퀀스 길이\n",
    "\n",
    "# 더미 입력 데이터 생성 (평균이 0이 아니고 분산이 1이 아닌 데이터)\n",
    "inputs = torch.randn(batch_size, seq_len, embedding_dim) * 3 + 2  # 평균=2, 표준편차=3\n",
    "print(f\"입력 텐서 형태: {inputs.shape}\")\n",
    "print(f\"입력 데이터 통계:\")\n",
    "print(f\"  - 전체 평균: {inputs.mean().item():.4f}\")\n",
    "print(f\"  - 전체 표준편차: {inputs.std().item():.4f}\")\n",
    "print(f\"  - 마지막 차원 평균 (첫 번째 샘플): {inputs[0, 0, :].mean().item():.4f}\")\n",
    "print(f\"  - 마지막 차원 표준편차 (첫 번째 샘플): {inputs[0, 0, :].std().item():.4f}\")\n",
    "\n",
    "# LayerNorm 인스턴스 생성\n",
    "layer_norm = LayerNorm(embedding_dim)\n",
    "\n",
    "# 모델 파라미터 확인\n",
    "print(f\"\\nLayerNorm 파라미터:\")\n",
    "print(f\"  - gamma 형태: {layer_norm.gamma.shape}\")\n",
    "print(f\"  - beta 형태: {layer_norm.beta.shape}\")\n",
    "print(f\"  - epsilon: {layer_norm.epsilon}\")\n",
    "\n",
    "# 순전파 실행\n",
    "outputs = layer_norm(inputs)\n",
    "\n",
    "print(f\"\\n출력 텐서 형태: {outputs.shape}\")\n",
    "print(f\"출력 데이터 통계:\")\n",
    "print(f\"  - 전체 평균: {outputs.mean().item():.4f}\")\n",
    "print(f\"  - 전체 표준편차: {outputs.std().item():.4f}\")\n",
    "print(f\"  - 마지막 차원 평균 (첫 번째 샘플): {outputs[0, 0, :].mean().item():.6f}\")\n",
    "print(f\"  - 마지막 차원 표준편차 (첫 번째 샘플): {outputs[0, 0, :].std().item():.6f}\")\n",
    "\n",
    "# PyTorch 내장 LayerNorm과 비교\n",
    "builtin_layer_norm = nn.LayerNorm(embedding_dim)\n",
    "builtin_outputs = builtin_layer_norm(inputs)\n",
    "\n",
    "print(f\"\\nPyTorch 내장 LayerNorm 비교:\")\n",
    "print(f\"  - 내장 LayerNorm 평균: {builtin_outputs[0, 0, :].mean().item():.6f}\")\n",
    "print(f\"  - 내장 LayerNorm 표준편차: {builtin_outputs[0, 0, :].std().item():.6f}\")\n",
    "\n",
    "# 정규화 효과 확인: 각 샘플의 각 위치에서 평균=0, 표준편차=1인지 확인\n",
    "print(f\"\\n정규화 검증 (처음 3개 샘플):\")\n",
    "for i in range(min(3, batch_size)):\n",
    "    for j in range(min(3, seq_len)):\n",
    "        sample_mean = outputs[i, j, :].mean().item()\n",
    "        sample_std = outputs[i, j, :].std().item()\n",
    "        print(f\"  샘플[{i}][{j}] - 평균: {sample_mean:.6f}, 표준편차: {sample_std:.6f}\")\n",
    "\n",
    "# 학습 가능한 파라미터 확인\n",
    "print(f\"\\n학습 가능한 파라미터:\")\n",
    "for name, param in layer_norm.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}, requires_grad={param.requires_grad}\")\n",
    "\n",
    "# 간단한 학습 예시\n",
    "print(f\"\\n학습 예시:\")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(layer_norm.parameters(), lr=0.01)\n",
    "\n",
    "# 가상의 target\n",
    "target = torch.randn_like(outputs)\n",
    "\n",
    "# 초기 loss\n",
    "initial_loss = criterion(outputs, target)\n",
    "print(f\"초기 Loss: {initial_loss.item():.4f}\")\n",
    "\n",
    "# 한 번의 학습 스텝\n",
    "optimizer.zero_grad()\n",
    "loss = criterion(layer_norm(inputs), target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"한 스텝 후 Loss: {loss.item():.4f}\")\n",
    "print(f\"Gamma 변화: {layer_norm.gamma[:5]}...\")  # 처음 5개 값만 출력\n",
    "print(f\"Beta 변화: {layer_norm.beta[:5]}...\")    # 처음 5개 값만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
