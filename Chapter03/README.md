# Chapter 3

## Chapter 3 Exercises

* CH3_BPE.ipynb [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/corazzon/designing-llm-apps/blob/main/Chapter03/CH3_BPE.ipynb)

## References

* "Lexical facts", https://oreil.ly/bkc2C
* https://oreil.ly/FxPcz
* Bittker, "@NYT_first_said (Twitter bot)", 2017, https://x.com/NYT_first_said
* https://platform.openai.com/tokenizer
* Hernandez et al., "Scaling Laws with Vocabulary: Larger Models Can Use Larger Vocabularies", 18 Jul 2024, https://arxiv.org/pdf/2407.13623v1
* OpenAI, "tiktoken: Fast BPE tokenization for OpenAI models", 2022, https://github.com/openai/tiktoken
* Clark et al., "CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation", 11 Mar 2021, https://arxiv.org/abs/2103.06874
* Xue et al., "ByT5: Towards a token-free future with pre-trained byte-to-byte models", 28 May 2021, https://arxiv.org/abs/2105.13626
* Tay et al., "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization", 23 Jun 2021, https://arxiv.org/abs/2106.12672
* https://oreil.ly/7JAyY
* Land et al., "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models", 8 May 2024, https://arxiv.org/pdf/2405.05417
* https://github.com/openai/tiktoken
* Taylor et al., "Galactica: A Large Language Model for Science", 16 Nov 2022, https://arxiv.org/pdf/2211.09085
* Goldman et al., "Unpacking Tokenization: Evaluating Text Compression and its Correlation
with Model Performance", 22 Jun 2024, https://arxiv.org/pdf/2403.06265
* Schmidt  et al., "Tokenization Is More Than Compression", 7 Oct 2024, https://arxiv.org/pdf/2402.18376
* Petrov et al., "Language Model Tokenizers Introduce Unfairness Between Languages", 17 May 2023, https://oreil.ly/ZATOQ