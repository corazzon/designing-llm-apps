# Chapter 6

## References 

* https://oreil.ly/3LX5X
* https://oreil.ly/VTiDa
* https://oreil.ly/4Z14D
* https://oreil.ly/7kdSO
* https://oreil.ly/0_0lt
* Jin et al., "Rethinking Learning Rate Tuning in the Era of Large Language Models”, 16 Sep 2023, https://arxiv.org/pdf/2309.08859 
* https://oreil.ly/_zdA9
* Loshchilov & Hutter., "SGDR: STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS", 3 May 2017, https://arxiv.org/pdf/1608.03983v5
* https://oreil.ly/i-R4I 
* "Evaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness ", 23 Apr 2023, https://arxiv.org/pdf/2304.11633
* Jain et al., "NEFTUNE: NOISY EMBEDDINGS IMPROVE INSTRUCTION FINETUNING", 10 Oct 2023, https://arxiv.org/pdf/2310.05914
* "Understanding Gradient Clipping (and How It Can Fix Exploding Gradients Problem)", https://oreil.ly/gH7L7
* "The intuitive idea behind Low-Rank Adaptation (LoRA)", Oct 2023, https://oreil.ly/_l91y
* Longpre et al., "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning", 31 Jan 2023, https://oreil.ly/e-MVh
* https://www.ontocord.ai/
* https://github.com/orhonovich/unnatural-instructions
* Zhou et al., "LIMA: Less Is More for Alignment", 18 May 2023, https://arxiv.org/abs/2305.11206
* https://github.com/yizhongw/self-instruct
* Xu et al., "WizardLM: Empowering large pre-trained language models to follow complex instructions", 24 Apr 2023, https://arxiv.org/abs/2304.12244
* https://github.com/XueFuzhao/InstructionWild
* Dettmers et al., "QLoRA: Efficient Finetuning of Quantized LLMs", 23 May 2023, https://arxiv.org/abs/2305.14314
* https://oreil.ly/SrXV-
* https://oreil.ly/DNKCv
* https://oreil.ly/DNKCv
* https://oreil.ly/WIyOq
* Wang et al., "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks ", 16 Apr 2022, https://arxiv.org/abs/2204.07705
* Wang et al., "SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions", 25 May 2023, https://arxiv.org/pdf/2212.10560
* Rogers et al., "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor", July 2023, https://aclanthology.org/2023.acl-long.806/
* Wang et al., "SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions", 25 May 2023, https://arxiv.org/pdf/2212.10560
* Zhou et al., "LIMA: Less Is More for Alignment", 18 May 2023, https://arxiv.org/abs/2305.11206
* Xu et al., "WizardLM: Empowering Large Language Models to Follow Complex Instructions", 24 Apr 2023, https://arxiv.org/abs/2304.12244
* https://oreil.ly/8wJ_o
