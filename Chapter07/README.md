# Chapter 7

## Notebooks

* [tutorial.ipynb](https://colab.research.google.com/github/your-username/designing-llm-apps/blob/main/Chapter07/finetune_tutorial/tutorial.ipynb)
* [tutorial_2b.ipynb](https://colab.research.google.com/github/your-username/designing-llm-apps/blob/main/Chapter07/finetune_tutorial/tutorial_2b.ipynb)

## References 

* Aharoni et al., "Unsupervised Domain Clusters in Pretrained Language Models", 1 May 2020, https://arxiv.org/pdf/2004.02105
* https://oreil.ly/llm-playbooks
* Jin et al., "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora", 19 Jul 2022, https://arxiv.org/pdf/2110.08534
* Winata et al., "Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning", July 2023, https://aclanthology.org/2023.findings-acl.48.pdf
* Jin et al., "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora", 19 Jul 2022, https://arxiv.org/pdf/2110.08534
* Winata et al., "Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning", July 2023, https://aclanthology.org/2023.findings-acl.48.pdf
* Gupta et al., "Continual Pre-Training of Large Language Models: How to (re)warm your
model?", 6 Sep 203, https://arxiv.org/pdf/2308.04014
* Gururangan et al., "DEMIX Layers: Disentangling Domains for Modular Language Modeling", 20 Aug 2021, https://arxiv.org/pdf/2108.05036
* https://docs.adapterhub.ml/
* https://oreil.ly/n1Pga
* Rücklé et al., "AdapterDrop: On the Efficiency of Adapters in Transformers", 5 Oct 2021, https://arxiv.org/pdf/2010.11918
* Lester et al., "The Power of Scale for Parameter-Efficient Prompt Tuning", 2 Sep 2014, https://arxiv.org/pdf/2104.08691
* Zaken et al., "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models", 5 Sep 2022, https://arxiv.org/pdf/2106.10199
* Zaken et al., "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models", 5 Sep 2022, https://arxiv.org/pdf/2106.10199
* Zhao et al., "Masking as an Efficient Alternative to Finetuning
for Pretrained Language Models", Apr 2020, https://oreil.ly/yhPdl
* Jiang et al., "LLM-BLENDER: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion", 30 Jun 2023, https://arxiv.org/pdf/2306.02561
* Don-Yehiya et al., "ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning", 13 Sep 2023, https://arxiv.org/pdf/2212.01378
* Chronopoulou et al., "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models", 28 Mar 2023, https://arxiv.org/pdf/2302.07027v3
* Wang et al., "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning", 2 Nov 2022, https://arxiv.org/pdf/2205.12410
