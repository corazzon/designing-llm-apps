{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Gemma chat model for Q&A\n",
    "\n",
    "In this tutorial we will finetune an instruction-tuned Gemma model to do Q&A on difficult medical questions. An instruction-tuned model is able to respond to text in a conversational manner, a format which is suitable for a wide variety of tasks. \n",
    "\n",
    "We will load a Q&A dataset and format it into a \"user\" and \"assistant\" chat format, where the user poses the questions and the model will answer these questions. We will then finetune the model to improve its answers.\n",
    "To facilitate the finetuning of such large models, we [load and finetune the model with 4-bit quantization](https://huggingface.co/docs/bitsandbytes/main/en/fsdp_qlora).\n",
    "\n",
    "This tutorial is loosely based on [this one](https://medium.com/the-ai-forum/instruction-fine-tuning-gemma-2b-on-medical-reasoning-and-convert-the-finetuned-model-into-gguf-844191f8d329). It is designed to work on an L4 GPU with 24GB VRAM with the instruction tuned variants of either [Gemma-2B](https://huggingface.co/google/gemma-2b-it) or [Gemma-2-9B](https://huggingface.co/google/gemma-2-9b-it).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "The config below exposes some key variables that affect the amount of training / eval data, how the model behaves, and important knobs for finetuning.\n",
    "\n",
    "GPU memory usage during finetuning is a prime concern for hobbyists. If you find yourself running out of memory, adjust some of these config variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# BASE_MODEL_ID = \"google/gemma-2b-it\"\n",
    "BASE_MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "\n",
    "# Dataset\n",
    "INPUT_LIMIT = 700  # lower this to limit GPU memory usage\n",
    "TRAIN_SIZE = 1000\n",
    "TEST_SIZE = 100\n",
    "EVAL_SIZE = 50\n",
    "SEED = 123\n",
    "\n",
    "# Generation / eval\n",
    "GENERATE_KWARGS = dict(\n",
    "    do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    temperature=1e-3,\n",
    ")\n",
    "EVAL_BATCH_SIZE = 4\n",
    "\n",
    "# Finetuning\n",
    "TRAIN_MAX_LENGTH = 512  # lower this to limit GPU memory usage\n",
    "TRAIN_NUM_EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = 4  # lower this to limit GPU memory usage, while slowing down training\n",
    "TRAIN_GRADIENT_ACCUMULATION_STEPS = 1  # equivalent to batch size, but without the computational speedup\n",
    "TRAIN_LOGGING_STEPS = 10\n",
    "EVAL_ACCUMULATION_STEPS = 4  # lower this to limit GPU memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gemma instruct model\n",
    "\n",
    "Notice that we are loading the model with 4-bit quantization. When finetuning Gemma-2, we need to use the `eager` attention implementation to stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040561df5a514d828e6f741756146151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    attn_implementation=\"eager\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Q & A\n",
    "\n",
    "The dataset we will finetune on is a processed version of [`medalpaca/medical_meadow_medqa`](https://huggingface.co/datasets/medalpaca/medical_meadow_medqa). We will skip discussing the data processing and just load and inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q:A 27-year-old previously healthy man presents to the clinic complaining of bloody diarrhea and abdominal pain. Sexual history reveals that he has sex with men and women and uses protection most of the time. He is febrile with all other vital signs within normal limits. Physical exam demonstrates tenderness to palpation of the right upper quadrant. Subsequent ultrasound shows a uniform cyst in the liver. In addition to draining the potential abscess and sending it for culture, appropriate medical therapy would involve which of the following?? \n",
      "{'A': 'Amphotericin', 'B': 'Nifurtimox', 'C': 'Supportive therapy', 'D': 'Sulfadiazine and pyrimethamine', 'E': 'Metronidazole and iodoquinol'},\n",
      "\n",
      "# output\n",
      "E: Metronidazole and iodoquinol\n"
     ]
    }
   ],
   "source": [
    "from medqa_data import load_train_test_data\n",
    "\n",
    "dataset = load_train_test_data(\n",
    "    train_size=TRAIN_SIZE,\n",
    "    test_size=TEST_SIZE,\n",
    "    seed=SEED,\n",
    "    input_limit=INPUT_LIMIT,\n",
    ")\n",
    "display(dataset)\n",
    "\n",
    "def print_sample(sample: dict[str, str]):\n",
    "    print(\"\\n\".join(f\"\\n# {k}\\n{v}\" for k, v in sample.items())[1:])\n",
    "\n",
    "print_sample(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is a multiple choice medical Q&A dataset. Each question comes with 5 possible answers to choose from.\n",
    "\n",
    "Let's pose this into a user-assistant chat format for the model and additionally, ask it to format the answer in JSON. The `<start_of_turn>user|assistant\\n` tags of the text to keep track of which parts correspond to the question vs. the model generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Q:A child is in the nursery one day after birth. A nurse notices a urine-like discharge being expressed through the umbilical stump. What two structures in the embryo are connected by the structure that failed to obliterate during the embryologic development of this child??\n",
      "{'A': 'Pulmonary artery - aorta', 'B': 'Bladder - yolk sac', 'C': 'Bladder - small bowel', 'D': 'Liver - umbilical vein', 'E': 'Kidney - large bowel'},\n",
      "Give your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "{\"option\": \"B\", \"option_text\": \"Bladder - yolk sac\"}  \n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Q:A child is in the nursery one day after birth. A nurse notices a urine-like discharge being expressed through the umbilical stump. What two structures in the embryo are connected by the structure that failed to obliterate during the embryologic development of this child??\n",
    "{'A': 'Pulmonary artery - aorta', 'B': 'Bladder - yolk sac', 'C': 'Bladder - small bowel', 'D': 'Liver - umbilical vein', 'E': 'Kidney - large bowel'},\n",
    "Give your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": question}]\n",
    "input_ids = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "output_ids = model.generate(input_ids, **GENERATE_KWARGS)\n",
    "input_ids = input_ids.to(\"cpu\")\n",
    "output_ids = output_ids.to(\"cpu\")\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are only interested in using the model for Q&A, so all our chats will consist of one turn from the user and assistant. However it is certainly possible to generate and finetune on multi-turn dialogue. For more on how to do that, see the [documentation](https://huggingface.co/docs/transformers/main/en/chat_templating).\n",
    "\n",
    "Next we will reformat the entire dataset into this chat format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q: A 27-year-old previously healthy man presents to the clinic complaining of bloody diarrhea and abdominal pain. Sexual history reveals that he has sex with men and women and uses protection most of the time. He is febrile with all other vital signs within normal limits. Physical exam demonstrates tenderness to palpation of the right upper quadrant. Subsequent ultrasound shows a uniform cyst in the liver. In addition to draining the potential abscess and sending it for culture, appropriate medical therapy would involve which of the following?? \n",
      "{'A': 'Amphotericin', 'B': 'Nifurtimox', 'C': 'Supportive therapy', 'D': 'Sulfadiazine and pyrimethamine', 'E': 'Metronidazole and iodoquinol'}\n",
      "Give your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.\n",
      "\n",
      "# output\n",
      "{\"option\": \"E\", \"text\": \"Metronidazole and iodoquinol\"}\n",
      "\n",
      "# true_label\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "from medqa_data import reformat_sample\n",
    "\n",
    "dataset = dataset.map(reformat_sample)\n",
    "train_data, test_data = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "print_sample(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we are almost ready to start finetuning. Before we do that, let's evaluate Gemma on our test set. \n",
    "\n",
    "If you are running Gemma-2-9B, you will notice that the performance is actually fairly good at ~60%, i.e. much higher than random! \n",
    "\n",
    "On the other hand, Gemma-2B literally performs at ~20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function create_predict.<locals>._batch_predict at 0x7ff30c0fc5e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a34d16897a40068d8ac1240e77b212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma accuracy: 58.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from medqa_data import create_predict\n",
    "\n",
    "batch_predict = create_predict(tokenizer, model, \"gemma\", batch=True, generate_kwargs=GENERATE_KWARGS)\n",
    "test_data = test_data.map(batch_predict, batched=True, batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "gemma_accuracy = (np.asarray(test_data[\"gemma_label\"]) == np.asarray(test_data[\"true_label\"])).mean()\n",
    "print(f\"Gemma accuracy: {round(gemma_accuracy * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised finetuning (SFT)\n",
    "\n",
    "Now we will finetune our model on the training data that we loaded. Since we are training the model to obey instructions, this is also a form of instruction tuning called [supervised finetuning (SFT)](https://huggingface.co/docs/trl/en/sft_trainer).\n",
    "\n",
    "First, we need to have a function that applies the model's chat template to the input questions and the expected outputs. Note that the implementation below does this in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start_of_turn>user\\nQ: A 70-year-old man with a long-standing history of diabetes mellitus type 2 and hypertension presents with complaints of constant wrist and shoulder pain. Currently, the patient undergoes hemodialysis 2 to 3 times a week and is on the transplant list for a kidney. The patient denies any recent traumas. Which of the following proteins is likely to be increased in his plasma, causing the patient’s late complaints?? \\n{\\'A\\': \\'Ig light chains\\', \\'B\\': \\'Amyloid A (AA)\\', \\'C\\': \\'Amyloid precursor protein\\', \\'D\\': \\'Urine tests will only be diagnostic of end-stage kidney disease\\', \\'E\\': \\'β2-microglobulin\\'}\\nGive your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"E\", \"text\": \"\\\\u03b22-microglobulin\"}<end_of_turn>\\n',\n",
       " '<start_of_turn>user\\nQ: A 60-year-old African-American female presents to your office complaining of dysuria, paresthesias, and blurry vision. Her body mass index is 37.2 kg/m2. Which of the following drugs would most significantly increase the levels of C-peptide in the blood when administered to this patient?? \\n{\\'A\\': \\'Metformin\\', \\'B\\': \\'Insulin\\', \\'C\\': \\'Glipizide\\', \\'D\\': \\'Acarbose\\', \\'E\\': \\'NPH\\'}\\nGive your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"C\", \"text\": \"Glipizide\"}<end_of_turn>\\n',\n",
       " '<start_of_turn>user\\nQ: A 71-year-old man with hypertension comes to the physician for a follow-up examination. Cardiovascular exam shows the point of maximal impulse to be in the mid-axillary line. A transthoracic echocardiogram shows concentric left ventricular hypertrophy with a normal right ventricle. Which of the following is the most likely underlying mechanism of this patient\\'s ventricular hypertrophy?? \\n{\\'A\\': \\'Accumulation of glycogen\\', \\'B\\': \\'Accumulation of protein fibrils\\', \\'C\\': \\'Deposition of endomyocardial collagen\\', \\'D\\': \\'Accumulation of sarcomeres in parallel\\', \\'E\\': \\'Infiltration of T lymphocytes\\'}\\nGive your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"D\", \"text\": \"Accumulation of sarcomeres in parallel\"}<end_of_turn>\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_chat_for_finetuning(samples: dict) -> list[str]:\n",
    "    chat_texts = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": input},\n",
    "                {\"role\": \"assistant\", \"content\": output},\n",
    "            ]\n",
    "            for input, output in zip(samples[\"input\"], samples[\"output\"])\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return [text.removeprefix(\"<bos>\") for text in chat_texts]\n",
    "\n",
    "display(create_chat_for_finetuning(train_data[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train the model using [parameter efficient finetuning](https://huggingface.co/docs/peft/en/index). This is again because our available GPU memory is a very limited resource. We will be using a [quantized low-rank adaptor (QLoRA)](https://huggingface.co/docs/bitsandbytes/main/en/fsdp_qlora), which allows us to train on a much smaller set of parameters than would otherwise be needed if we were to do full finetuning.\n",
    "\n",
    "The adaptor essentially sits on top of the base Gemma model, and we will be training on a model that contains both the base model and adaptor components (but only the adaptor's weights get updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 216,072,192 | total: 9,457,778,176 | Percentage: 2.2846%\n"
     ]
    }
   ],
   "source": [
    "# pip install peft\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable:,} | total: {total:,} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's finetune! Note the various flags that need to be turned on before training starts, e.g. `model.gradient_checkpointing_enable()` or `tokenizer.padding_size = right`. Working with LLMs and neural networks in general often involves dealing with many of these technical details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d839ee33e07c4202a5d4d048c14c2baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 20:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.130881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.097202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.089198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.089355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.087239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.082603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.075994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.074501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.074349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.072865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.071786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.071941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.073212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.069591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.066609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.065880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.066203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.066476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.066061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.065232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.064805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.064554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.064589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.064364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.064638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'global_step': 250,\n",
       " 'training_loss': 0.07853067517280579,\n",
       " 'metrics': {'train_runtime': 1239.651,\n",
       "  'train_samples_per_second': 0.807,\n",
       "  'train_steps_per_second': 0.202,\n",
       "  'total_flos': 1.17394608823296e+16,\n",
       "  'train_loss': 0.07853067517280579,\n",
       "  'epoch': 1.0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.use_cache=False\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    instruction_template=\"<start_of_turn>user\\n\",\n",
    "    response_template=\"<start_of_turn>model\\n\",\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=SFTConfig(\n",
    "        output_dir=\"/tmp/finetuned_gemma_2b\",\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=TRAIN_GRADIENT_ACCUMULATION_STEPS,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs=dict(use_reentrant=False),\n",
    "        max_seq_length=TRAIN_MAX_LENGTH,\n",
    "        num_train_epochs=TRAIN_NUM_EPOCHS,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=TRAIN_LOGGING_STEPS,\n",
    "        eval_steps=TRAIN_LOGGING_STEPS,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_accumulation_steps=EVAL_ACCUMULATION_STEPS,\n",
    "    ),\n",
    "    data_collator=collator,\n",
    "    eval_dataset=test_data.select(range(min(len(test_data), EVAL_SIZE))),\n",
    "    formatting_func=create_chat_for_finetuning,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=train_data,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "display(train_result._asdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we finished training! Now let's reset all the flags toggled for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache=True\n",
    "model.gradient_checkpointing_disable()\n",
    "model.eval()\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and evaluate the finetuned model\n",
    "\n",
    "After training the LoRA adapter has not yet been merged with the base Gemma model. This will make it run a lot slower. To merge the LoRA adapter, we will follow these steps:\n",
    "- https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload\n",
    "- https://discuss.huggingface.co/t/help-with-merging-lora-weights-back-into-base-model/40968/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc71e736628438a8ba187c802863830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "trainer.model.save_pretrained(\"models/lora_adapter\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"models/lora_adapter\").merge_and_unload()\n",
    "model.save_pretrained(\"models/finetuned_model\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's clean up all our prior models from GPU memory, and load the merged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6b46ce266f4339a802d70deaefae02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "try: del model\n",
    "except NameError: pass\n",
    "try: del trainer\n",
    "except NameError: pass\n",
    "try: del base_model\n",
    "except NameError: pass\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"models/finetuned_model\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will run the finetuned model on our test data once more and compare the performance.\n",
    "\n",
    "If you ran the tutorial using Gemma-2B, the performance would've increased from ~19% to ~25%. For Gemma-2-9B, the increase will be from ~58% to ~62%. So, the improvement is higher on the Gemma-2B, but Gemma-2-9B is significantly more capable overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1e50e0523b4487a1728de1ec16aaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma accuracy: 58.0%\n",
      "Finetuned accuracy: 62.0%\n"
     ]
    }
   ],
   "source": [
    "batch_predict = create_predict(tokenizer, model, \"finetuned\", batch=True, generate_kwargs=GENERATE_KWARGS)\n",
    "test_data = test_data.map(batch_predict, batched=True, batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "finetuned_accuracy = (np.asarray(test_data[\"finetuned_label\"]) == np.asarray(test_data[\"true_label\"])).mean()\n",
    "print(f\"Gemma accuracy: {round(gemma_accuracy * 100, 1)}%\")\n",
    "print(f\"Finetuned accuracy: {round(finetuned_accuracy * 100, 1)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
